{"cells":[{"cell_type":"markdown","metadata":{"id":"B5MM-4KYH2T3"},"source":["# Install and import libraries and Data"]},{"cell_type":"code","source":["!pip3 install torchvision==0.10.0 --quiet"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rslXNOYa-M_4","executionInfo":{"status":"ok","timestamp":1651584852362,"user_tz":240,"elapsed":110033,"user":{"displayName":"Katelyn Morrison","userId":"11747250929607801523"}},"outputId":"e734ef43-4238-4b30-e3e1-8447d4877099"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[K     |████████████████████████████████| 22.1 MB 79.2 MB/s \n","\u001b[K     |████████████████████████████████| 831.4 MB 2.7 kB/s \n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchtext 0.12.0 requires torch==1.11.0, but you have torch 1.9.0 which is incompatible.\n","torchaudio 0.11.0+cu113 requires torch==1.11.0, but you have torch 1.9.0 which is incompatible.\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7q60Cm1CpZlH","executionInfo":{"status":"ok","timestamp":1651584887795,"user_tz":240,"elapsed":31943,"user":{"displayName":"Katelyn Morrison","userId":"11747250929607801523"}},"outputId":"020bcefc-9f14-4c38-9617-f758a007d14f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yB9xtN0mItYs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651584902572,"user_tz":240,"elapsed":14783,"user":{"displayName":"Katelyn Morrison","userId":"11747250929607801523"}},"outputId":"d0ff860c-86d6-4f79-cd35-c1c0b6405eb7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'pytorch-grad-cam'...\n","remote: Enumerating objects: 796, done.\u001b[K\n","remote: Counting objects: 100% (94/94), done.\u001b[K\n","remote: Compressing objects: 100% (59/59), done.\u001b[K\n","remote: Total 796 (delta 39), reused 65 (delta 32), pack-reused 702\u001b[K\n","Receiving objects: 100% (796/796), 17.09 MiB | 10.68 MiB/s, done.\n","Resolving deltas: 100% (418/418), done.\n","/bin/bash: line 0: cd: pytorch_grad_cam: No such file or directory\n","\u001b[K     |████████████████████████████████| 4.5 MB 36.4 MB/s \n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for grad-cam (PEP 517) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["!git clone https://github.com/katelyn98/pytorch-grad-cam.git\n","!cd pytorch_grad_cam && pip install -r requirements.txt\n","!pip install requests --quiet\n","# !pip install grad-cam --quiet"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZJdJlGy2I6Od","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651584905805,"user_tz":240,"elapsed":3238,"user":{"displayName":"Katelyn Morrison","userId":"11747250929607801523"}},"outputId":"258fbd0d-d9e9-4b17-8554-3984aa0791fc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'DeepGaze'...\n","remote: Enumerating objects: 66, done.\u001b[K\n","remote: Counting objects: 100% (66/66), done.\u001b[K\n","remote: Compressing objects: 100% (40/40), done.\u001b[K\n","remote: Total 66 (delta 36), reused 54 (delta 24), pack-reused 0\u001b[K\n","Unpacking objects: 100% (66/66), done.\n","--2022-05-03 13:35:03--  https://github.com/matthias-k/DeepGaze/releases/download/v1.0.0/centerbias_mit1003.npy\n","Resolving github.com (github.com)... 20.205.243.166\n","Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/372933216/3c267f80-c32e-11eb-9f03-c6381f7da54a?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220503%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220503T133504Z&X-Amz-Expires=300&X-Amz-Signature=90052030c94ea386de8dca4e50f1ab08f0082bf54532bc8a51ad64884fba96cd&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=372933216&response-content-disposition=attachment%3B%20filename%3Dcenterbias_mit1003.npy&response-content-type=application%2Foctet-stream [following]\n","--2022-05-03 13:35:04--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/372933216/3c267f80-c32e-11eb-9f03-c6381f7da54a?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220503%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220503T133504Z&X-Amz-Expires=300&X-Amz-Signature=90052030c94ea386de8dca4e50f1ab08f0082bf54532bc8a51ad64884fba96cd&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=372933216&response-content-disposition=attachment%3B%20filename%3Dcenterbias_mit1003.npy&response-content-type=application%2Foctet-stream\n","Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 8388688 (8.0M) [application/octet-stream]\n","Saving to: ‘centerbias_mit1003.npy’\n","\n","centerbias_mit1003. 100%[===================>]   8.00M  11.6MB/s    in 0.7s    \n","\n","2022-05-03 13:35:05 (11.6 MB/s) - ‘centerbias_mit1003.npy’ saved [8388688/8388688]\n","\n"]}],"source":["!git clone https://github.com/Gkao03/DeepGaze.git\n","!mv DeepGaze/deepgaze_pytorch ./\n","!mv DeepGaze/setup.py ./\n","!rm -rf DeepGaze\n","!wget https://github.com/matthias-k/DeepGaze/releases/download/v1.0.0/centerbias_mit1003.npy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zcK-Z85XICaG"},"outputs":[],"source":["import torchvision.transforms as transforms\n","import torchvision\n","from PIL import Image\n","from torch.utils.data import Dataset, DataLoader\n","import glob\n","import os\n","import torch\n","import warnings\n","warnings.filterwarnings('ignore')\n","warnings.simplefilter('ignore')  \n","import cv2\n","import numpy as np\n","import math\n","import requests\n","from scipy.misc import face\n","from scipy.ndimage import zoom\n","from scipy.special import logsumexp\n","import matplotlib.pyplot as plt\n","from google.colab.patches import cv2_imshow\n","import deepgaze_pytorch\n","\n","%cd '/content/pytorch-grad-cam/'\n","from pytorch_grad_cam import EigenCAM\n","from pytorch_grad_cam.utils.image import show_cam_on_image, scale_cam_image\n","from pytorch_grad_cam.utils.model_targets import FasterRCNNBoxScoreTarget\n","\n","DEVICE = 'cuda'"]},{"cell_type":"markdown","metadata":{"id":"NnKejgDmIK0i"},"source":["#Data Loader"]},{"cell_type":"markdown","source":["Download our trainval/test split of PASCAL2012VOC [here](https://drive.google.com/file/d/1p2ZkFiNQR0WbE28x8ukLzCUBjUy9XXM4/view?usp=sharing) and upload it to your google drive. Then copy the zip file to the Colab File System and unzip it. Be sure to change the path to original_data to match where you put it."],"metadata":{"id":"6evvLCSDsjdM"}},{"cell_type":"code","source":["!cp /content/drive/MyDrive/VLR_Project/data/PASCAL2012/original_data.zip /content/ && unzip /content/original_data.zip"],"metadata":{"id":"bygtlGWdWq6i"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HQuHQ6NAIKfq"},"outputs":[],"source":["data_pth = \"/content/original_data/JPEGImages/trainval\"\n","train_path = \"/content/original_data/JPEGImages/trainval\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HY3plboeIWGR"},"outputs":[],"source":["train_transform = transforms.Compose([transforms.Resize((512, 512)), \n","                                      transforms.ToTensor(),\n","                                      ])\n","\n","class MyDataset(Dataset):\n","    def __init__(self, image_dir, mask_dir, transform=None):\n","        self.image_dir = image_dir\n","        self.transform = transform\n","\n","        self.image_paths = []\n","        for file in os.listdir(image_dir):\n","            if file.endswith(\".jpg\"):\n","                self.image_paths.append(os.path.join(data_pth, file))\n","\n","        if mask_dir is not None:\n","          self.mask_dir = mask_dir\n","          self.mask_paths = []\n","          for file in os.listdir(mask_dir):\n","              if file.endswith(\".jpg\"):\n","                  self.mask_paths.append(os.path.join(mask_pth, file))\n","        else:\n","          self.mask_dir = None\n","\n","    def __len__(self):\n","        return len(self.image_paths)\n","\n","    def __getitem__(self, idx):\n","        img_path = self.image_paths[idx]\n","        img = Image.open(img_path)\n","        tensor = self.transform(img).unsqueeze(0)\n","\n","        if self.mask_dir is not None:\n","          mask_path = self.mask_paths[idx]\n","          mask = np.array(Image.open(mask_path).convert(\"L\"))\n","\n","          mask = cv2.resize(mask, (512, 512))\n","        else:\n","          mask = None\n","\n","        return img, tensor, mask, img_path\n","        \n","\n","def collate_fn(sample):\n","    # only with batch size=1\n","    img, tensor, mask, img_path = sample[0]\n","    return img, tensor, mask, img_path\n","\n","# #for inpainting\n","train_dataset = MyDataset(image_dir=train_path, mask_dir=None, transform=train_transform)\n","train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, collate_fn=collate_fn)"]},{"cell_type":"markdown","metadata":{"id":"lbMhpLN0NJzx"},"source":["# Helper Functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vL6DNu_ENLse"},"outputs":[],"source":["def resizer(x, res=512):  # input is np array\n","    # x = x.detach().cpu().numpy()\n","    x = cv2.resize(x, (res, res))\n","    rgb_img = x.copy()\n","    x = np.float32(x) / 255\n","    # transform = transforms.ToTensor()\n","    # tensor = transform(img).unsqueeze(0)\n","    return x\n","\n","def normalize_data(data):  # np array input\n","    return (data - np.min(data)) / (np.max(data) - np.min(data))"]},{"cell_type":"code","source":["def erase_black(attributions, img_orig, threshold = 97.5):\n","  mask = attributions <= np.percentile(attributions, threshold)\n","  im_mask = np.array(img_orig)\n","  im_mask[~mask] = 0\n","  return im_mask"],"metadata":{"id":"wNgw0RrO_o-k"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DLbZkinFJVLx"},"source":["# Metrics"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EUPiEWVYJWrr"},"outputs":[],"source":["def calcMAE(P, Y):\n","    # takes two np arrays of the same shape\n","    assert(P.shape == Y.shape)\n","    W, H = P.shape\n","    # print(\"DEEPGAZE: \", P)\n","    # print(\"REGULAR: \", Y)\n","    abs_diff = np.abs(P - Y)\n","    return np.sum(abs_diff) / (W * H)\n","\n","def calcIOU(a, b, percentile=90):  # default 90% threshold \n","    # takes two np arrays normalized to 0-1 of same shape\n","    assert(a.shape == b.shape)\n","    a_bool = a >= np.percentile(a, percentile)\n","    b_bool = b >= np.percentile(b, percentile)\n","\n","    overlap = a_bool & b_bool # Logical AND\n","    union = a_bool | b_bool # Logical OR\n","\n","    IOU = overlap.sum() / float(union.sum())\n","    return IOU"]},{"cell_type":"markdown","metadata":{"id":"W0F-tEftJG8A"},"source":["# Deep Gaze 2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h5zdfhayIgam","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651619399595,"user_tz":240,"elapsed":7767,"user":{"displayName":"Katelyn Morrison","userId":"11747250929607801523"}},"outputId":"bc8c2667-a913-4089-b629-a1fa55187af9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded pretrained weights for efficientnet-b5\n"]},{"output_type":"stream","name":"stderr","text":["Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n","Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n"]}],"source":["# you can use DeepGazeI or DeepGazeIIE\n","torch.hub._validate_not_a_forked_repo=lambda a,b,c: True\n","deepgaze_model = deepgaze_pytorch.DeepGazeIIE(pretrained=True).to(DEVICE)"]},{"cell_type":"markdown","source":["# Faster R-CNN"],"metadata":{"id":"5Q3UbMT_L3SB"}},{"cell_type":"code","source":["frcnn_model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n","frcnn_model.eval().cuda()\n","\n","target_layers = [frcnn_model.backbone.body.layer4] #ssd\n","cam = EigenCAM(frcnn_model,\n","               target_layers, \n","               use_cuda=torch.cuda.is_available())"],"metadata":{"id":"WLP1bvTXL5HG","colab":{"base_uri":"https://localhost:8080/","height":87,"referenced_widgets":["a0559d6d44884e0a957a0c088ab0c9f7","129245fa67e64b9dbb0e68e01ae3f1df","2c056eab344e45e984274a8621cc2fe0","bea4822495444392987600ac5e79bca1","e6906b6bf70240fdaf6c72aaeb312480","cd5d73287cf948418a6f4d9227551780","1c85b714bcbf432baf851a52f7def0bb","70cc39d3c46345148cec154965d72ea9","61476578de1445d1b7e378dcd0b061ca","c5a7dd19d6644c2788113e98f0e3dc2c","c5fcd0c6c46c464e979cc165fa32e922"]},"executionInfo":{"status":"ok","timestamp":1651619405201,"user_tz":240,"elapsed":5611,"user":{"displayName":"Katelyn Morrison","userId":"11747250929607801523"}},"outputId":"fb1b324c-432e-445d-f33a-476bf22a2e4b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0.00/160M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0559d6d44884e0a957a0c088ab0c9f7"}},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"oQzoySHIJlDf"},"source":["# Evaluate"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RqoM5rLfL3ZI"},"outputs":[],"source":["#prediction function for SSD model\n","coco_names = ['__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', \\\n","              'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', \n","              'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', \n","              'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella',\n","              'N/A', 'N/A', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard',\n","              'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard',\n","              'surfboard', 'tennis racket', 'bottle', 'N/A', 'wine glass', 'cup', 'fork',\n","              'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange',\n","              'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch',\n","              'potted plant', 'bed', 'N/A', 'dining table', 'N/A', 'N/A', 'toilet',\n","              'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave',\n","              'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book', 'clock', 'vase',\n","              'scissors', 'teddy bear', 'hair drier', 'toothbrush']\n","\n","\n","def predict(input_tensor, model, device, detection_threshold):\n","    outputs = model(input_tensor)\n","    pred_classes = [coco_names[i] for i in outputs[0]['labels'].cpu().numpy()]\n","    pred_labels = outputs[0]['labels'].cpu().numpy()\n","    pred_scores = outputs[0]['scores'].detach().cpu().numpy()\n","    pred_bboxes = outputs[0]['boxes'].detach().cpu().numpy()\n","    \n","    boxes, classes, labels, indices = [], [], [], []\n","    for index in range(len(pred_scores)):\n","        if pred_scores[index] >= detection_threshold:\n","            boxes.append(pred_bboxes[index].astype(np.int32))\n","            classes.append(pred_classes[index])\n","            labels.append(pred_labels[index])\n","            indices.append(index)\n","    boxes = np.int32(boxes)\n","    return boxes, classes, labels, indices"]},{"cell_type":"markdown","source":["# Inpaint Setup"],"metadata":{"id":"sHTZ1s16MG84"}},{"cell_type":"markdown","metadata":{"id":"oz8jwHP9qTSZ"},"source":["## Evaluation"]},{"cell_type":"markdown","source":["Main loop for selectively erasing regions in images. You can change the mae threshold at the end of this code cell where it says `if iou_deepgaze <= 0.1`. You can change the threshold by passing a value into `erase_black()` function.\n","\n","Be sure to change the file paths for where images are being saved."],"metadata":{"id":"WhtQAJvLtC82"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"sdK8elMyJkNU"},"outputs":[],"source":["mae_deepgaze_list = []\n","iou_deepgaze_list = []\n","\n","for batch_idx, (img, tensor, mask, img_path) in enumerate(train_loader):\n","    print(img_path)\n","    print(img_path.split(\"/\")[-1].split(\".\")[0])\n","    # print(tensor.shape)\n","    img = np.array(img)\n","    img = cv2.resize(img, (512, 512))\n","    img_arr = np.float32(img) / 255\n","    if mask is not None:\n","      mask_arr = np.float32(mask) / 255\n","    filename = os.path.basename(img_path)\n","    tensor = tensor.cuda()\n","    #SSD\n","    boxes, classes, labels, indices = predict(tensor, frcnn_model, DEVICE, 0.9)\n","    targets = [FasterRCNNBoxScoreTarget(labels=labels, bounding_boxes=boxes)]\n","\n","    grayscale_cam = cam(tensor, targets=targets)\n","    grayscale_cam = grayscale_cam[0, :]\n","    cam_image = show_cam_on_image(img_arr, grayscale_cam, use_rgb=True)\n","\n","    # plt.imshow(img)\n","    # plt.imshow(grayscale_cam, cmap=\"plasma\", alpha=0.7)\n","    # plt.savefig('/content/drive/MyDrive/VLR_Project/data/PASCAL2012/SaliencyMaps/SSD/'+img_path.split(\"/\")[-1].split(\".\")[0]+'.png')\n","    # plt.show()\n","\n","    # deepgaze\n","    centerbias_template = np.load('centerbias_mit1003.npy')\n","    centerbias = zoom(centerbias_template, (img.shape[0]/centerbias_template.shape[0], img.shape[1]/centerbias_template.shape[1]), order=0, mode='nearest')\n","    centerbias -= logsumexp(centerbias)\n","    centerbias_tensor = torch.tensor([centerbias]).to(DEVICE)\n","\n","    log_density_prediction = deepgaze_model(tensor, centerbias_tensor)\n","    log_density_prediction_squeezed = log_density_prediction.squeeze()\n","    out_deepgaze = resizer(np.exp(log_density_prediction_squeezed.detach().cpu().numpy()))\n","    out_deepgaze = normalize_data(out_deepgaze)\n","\n","    #metrics calculation for deepgaze\n","    mae_deepgaze = calcMAE(out_deepgaze, grayscale_cam)\n","    mae_deepgaze_list.append(mae_deepgaze)\n","\n","    iou_deepgaze = calcIOU(out_deepgaze, grayscale_cam)\n","    iou_deepgaze_list.append(iou_deepgaze)\n","\n","    plt.clf()\n","    inpainted_img = erase_black(grayscale_cam, img)\n","    # plt.imshow(inpainted_img)\n","    # plt.savefig('/content/drive/MyDrive/VLR_Project/data/PASCAL2012/PASCAL-INPAINTED/SSD/All/'+img_path.split(\"/\")[-1].split(\".\")[0]+'.jpg')\n","\n","    if iou_deepgaze <= 0.1:\n","      plt.clf()\n","      plt.imshow(inpainted_img)\n","      plt.savefig('/content/drive/MyDrive/VLR_Project/data/PASCAL2012/PASCAL-INPAINTED/SSD/IoUDeepGaze2.5/'+img_path.split(\"/\")[-1].split(\".\")[0]+'.jpg')\n","\n","    print(f\"file {filename}. Progress {batch_idx+1}/{len(train_loader)}\")"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"Selective Erasing.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"a0559d6d44884e0a957a0c088ab0c9f7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_129245fa67e64b9dbb0e68e01ae3f1df","IPY_MODEL_2c056eab344e45e984274a8621cc2fe0","IPY_MODEL_bea4822495444392987600ac5e79bca1"],"layout":"IPY_MODEL_e6906b6bf70240fdaf6c72aaeb312480"}},"129245fa67e64b9dbb0e68e01ae3f1df":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cd5d73287cf948418a6f4d9227551780","placeholder":"​","style":"IPY_MODEL_1c85b714bcbf432baf851a52f7def0bb","value":"100%"}},"2c056eab344e45e984274a8621cc2fe0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_70cc39d3c46345148cec154965d72ea9","max":167502836,"min":0,"orientation":"horizontal","style":"IPY_MODEL_61476578de1445d1b7e378dcd0b061ca","value":167502836}},"bea4822495444392987600ac5e79bca1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c5a7dd19d6644c2788113e98f0e3dc2c","placeholder":"​","style":"IPY_MODEL_c5fcd0c6c46c464e979cc165fa32e922","value":" 160M/160M [00:04&lt;00:00, 44.3MB/s]"}},"e6906b6bf70240fdaf6c72aaeb312480":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cd5d73287cf948418a6f4d9227551780":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1c85b714bcbf432baf851a52f7def0bb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"70cc39d3c46345148cec154965d72ea9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"61476578de1445d1b7e378dcd0b061ca":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c5a7dd19d6644c2788113e98f0e3dc2c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c5fcd0c6c46c464e979cc165fa32e922":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}
