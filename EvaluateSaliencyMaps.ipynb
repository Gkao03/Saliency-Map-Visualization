{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5MM-4KYH2T3"
      },
      "source": [
        "# Install and import libraries and Data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install torchvision==0.10.0 --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rslXNOYa-M_4",
        "outputId": "84d0d847-c413-4f96-ab16-9f444aa079ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 22.1 MB 1.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 831.4 MB 2.7 kB/s \n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.12.0 requires torch==1.11.0, but you have torch 1.9.0 which is incompatible.\n",
            "torchaudio 0.11.0+cu113 requires torch==1.11.0, but you have torch 1.9.0 which is incompatible.\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7q60Cm1CpZlH",
        "outputId": "aec96dd2-f9ca-47d0-ee91-54f5bea17bc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yB9xtN0mItYs"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/katelyn98/pytorch-grad-cam.git\n",
        "!cd pytorch_grad_cam && pip install -r requirements.txt\n",
        "!pip install requests --quiet\n",
        "# !pip install grad-cam --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZJdJlGy2I6Od"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/Gkao03/DeepGaze.git\n",
        "!mv DeepGaze/deepgaze_pytorch ./\n",
        "!mv DeepGaze/setup.py ./\n",
        "!rm -rf DeepGaze\n",
        "!wget https://github.com/matthias-k/DeepGaze/releases/download/v1.0.0/centerbias_mit1003.npy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zcK-Z85XICaG"
      },
      "outputs": [],
      "source": [
        "import torchvision.transforms as transforms\n",
        "import torchvision\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import glob\n",
        "import os\n",
        "import torch\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "warnings.simplefilter('ignore')  \n",
        "import cv2\n",
        "import numpy as np\n",
        "import math\n",
        "import requests\n",
        "from scipy.misc import face\n",
        "from scipy.ndimage import zoom\n",
        "from scipy.special import logsumexp\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab.patches import cv2_imshow\n",
        "import deepgaze_pytorch\n",
        "\n",
        "%cd '/content/pytorch-grad-cam/'\n",
        "from pytorch_grad_cam import EigenCAM\n",
        "from pytorch_grad_cam.utils.image import show_cam_on_image, scale_cam_image\n",
        "from pytorch_grad_cam.utils.model_targets import FasterRCNNBoxScoreTarget\n",
        "\n",
        "DEVICE = 'cuda'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnKejgDmIK0i"
      },
      "source": [
        "#Data Loader"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download our trainval/test split of PASCAL2012VOC [here](https://drive.google.com/file/d/1p2ZkFiNQR0WbE28x8ukLzCUBjUy9XXM4/view?usp=sharing) and upload it to your google drive. Then copy the zip file to the Colab File System and unzip it. Be sure to change the path to original_data to match where you put it. Note: If you want to evaluate models on MIT1003 dataset, please download the images from [here](https://drive.google.com/file/d/1d9XhYXq7Wj6ANmKgvbmYDHTt0xtCGZxE/view?usp=sharing). You can also download the MIT1003 dataset [here](http://people.csail.mit.edu/tjudd/WherePeopleLook/index.html) - the images are in All Stimuli."
      ],
      "metadata": {
        "id": "6evvLCSDsjdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/MyDrive/VLR_Project/data/PASCAL2012/original_data.zip /content/ && unzip /content/original_data.zip"
      ],
      "metadata": {
        "id": "bygtlGWdWq6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HQuHQ6NAIKfq"
      },
      "outputs": [],
      "source": [
        "data_pth = \"/content/original_data/JPEGImages/test\"\n",
        "mask_pth = \"/content/original_data/HumanAttentionMasks/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HY3plboeIWGR"
      },
      "outputs": [],
      "source": [
        "train_transform = transforms.Compose([transforms.Resize((512, 512)), \n",
        "                                      transforms.ToTensor(),\n",
        "                                      ])\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, image_dir, mask_dir, transform=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.transform = transform\n",
        "\n",
        "        self.image_paths = []\n",
        "        for file in os.listdir(image_dir):\n",
        "            if file.endswith(\".jpg\"):\n",
        "                self.image_paths.append(os.path.join(data_pth, file))\n",
        "\n",
        "        if mask_dir is not None:\n",
        "          self.mask_dir = mask_dir\n",
        "          self.mask_paths = []\n",
        "          for file in os.listdir(mask_dir):\n",
        "              if file.endswith(\".jpg\"):\n",
        "                  self.mask_paths.append(os.path.join(mask_pth, file))\n",
        "        else:\n",
        "          self.mask_dir = None\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        img = Image.open(img_path)\n",
        "        tensor = self.transform(img).unsqueeze(0)\n",
        "\n",
        "        if self.mask_dir is not None:\n",
        "          mask_path = self.mask_paths[idx]\n",
        "          mask = np.array(Image.open(mask_path).convert(\"L\"))\n",
        "\n",
        "          mask = cv2.resize(mask, (512, 512))\n",
        "        else:\n",
        "          mask = None\n",
        "\n",
        "        return img, tensor, mask, img_path\n",
        "        \n",
        "\n",
        "def collate_fn(sample):\n",
        "    # only with batch size=1\n",
        "    img, tensor, mask, img_path = sample[0]\n",
        "    return img, tensor, mask, img_path\n",
        "\n",
        "#for test and evaluation\n",
        "train_dataset = MyDataset(image_dir=data_pth, mask_dir=mask_pth, transform=train_transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbMhpLN0NJzx"
      },
      "source": [
        "# Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vL6DNu_ENLse"
      },
      "outputs": [],
      "source": [
        "def resizer(x, res=512):  # input is np array\n",
        "    # x = x.detach().cpu().numpy()\n",
        "    x = cv2.resize(x, (res, res))\n",
        "    rgb_img = x.copy()\n",
        "    x = np.float32(x) / 255\n",
        "    # transform = transforms.ToTensor()\n",
        "    # tensor = transform(img).unsqueeze(0)\n",
        "    return x\n",
        "\n",
        "def normalize_data(data):  # np array input\n",
        "    return (data - np.min(data)) / (np.max(data) - np.min(data))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def inpaint_black(attributions, img_orig, threshold = 95):\n",
        "  mask = attributions <= np.percentile(attributions, threshold)\n",
        "  im_mask = np.array(img_orig)\n",
        "  im_mask[~mask] = 0\n",
        "  return im_mask"
      ],
      "metadata": {
        "id": "wNgw0RrO_o-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLbZkinFJVLx"
      },
      "source": [
        "# Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EUPiEWVYJWrr"
      },
      "outputs": [],
      "source": [
        "def calcMAE(P, Y):\n",
        "    # takes two np arrays of the same shape\n",
        "    assert(P.shape == Y.shape)\n",
        "    W, H = P.shape\n",
        "    # print(\"DEEPGAZE: \", P)\n",
        "    # print(\"REGULAR: \", Y)\n",
        "    abs_diff = np.abs(P - Y)\n",
        "    return np.sum(abs_diff) / (W * H)\n",
        "\n",
        "def calcIOU(a, b, percentile=90):  # default 90% threshold \n",
        "    # takes two np arrays normalized to 0-1 of same shape\n",
        "    assert(a.shape == b.shape)\n",
        "    a_bool = a >= np.percentile(a, percentile)\n",
        "    b_bool = b >= np.percentile(b, percentile)\n",
        "\n",
        "    overlap = a_bool & b_bool # Logical AND\n",
        "    union = a_bool | b_bool # Logical OR\n",
        "\n",
        "    IOU = overlap.sum() / float(union.sum())\n",
        "    return IOU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0F-tEftJG8A"
      },
      "source": [
        "# Deep Gaze 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5zdfhayIgam",
        "outputId": "14aa6bce-94f6-4dfc-dc8a-b093960124d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded pretrained weights for efficientnet-b5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n",
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n"
          ]
        }
      ],
      "source": [
        "# you can use DeepGazeI or DeepGazeIIE\n",
        "torch.hub._validate_not_a_forked_repo=lambda a,b,c: True\n",
        "deepgaze_model = deepgaze_pytorch.DeepGazeIIE(pretrained=True).to(DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Faster R-CNN"
      ],
      "metadata": {
        "id": "5Q3UbMT_L3SB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "frcnn_model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "frcnn_model.eval().cuda()\n",
        "\n",
        "target_layers = [frcnn_model.backbone.body.layer4] #ssd\n",
        "cam = EigenCAM(frcnn_model,\n",
        "               target_layers, \n",
        "               use_cuda=torch.cuda.is_available())"
      ],
      "metadata": {
        "id": "WLP1bvTXL5HG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87,
          "referenced_widgets": [
            "a0559d6d44884e0a957a0c088ab0c9f7",
            "129245fa67e64b9dbb0e68e01ae3f1df",
            "2c056eab344e45e984274a8621cc2fe0",
            "bea4822495444392987600ac5e79bca1",
            "e6906b6bf70240fdaf6c72aaeb312480",
            "cd5d73287cf948418a6f4d9227551780",
            "1c85b714bcbf432baf851a52f7def0bb",
            "70cc39d3c46345148cec154965d72ea9",
            "61476578de1445d1b7e378dcd0b061ca",
            "c5a7dd19d6644c2788113e98f0e3dc2c",
            "c5fcd0c6c46c464e979cc165fa32e922"
          ]
        },
        "outputId": "fb1b324c-432e-445d-f33a-476bf22a2e4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/160M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a0559d6d44884e0a957a0c088ab0c9f7"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7yALbZmJizx"
      },
      "source": [
        "# SSD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8gvhQYWNJjya"
      },
      "outputs": [],
      "source": [
        "ssd_model = torchvision.models.detection.ssd300_vgg16(pretrained=True)\n",
        "ssd_model.eval().cuda()\n",
        "\n",
        "target_layers = [ssd_model.backbone.extra[1]] #ssd\n",
        "cam = EigenCAM(ssd_model,\n",
        "               target_layers, \n",
        "               use_cuda=torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQzoySHIJlDf"
      },
      "source": [
        "# Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "coco_names = ['__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', \\\n",
        "              'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', \n",
        "              'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', \n",
        "              'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella',\n",
        "              'N/A', 'N/A', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard',\n",
        "              'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard',\n",
        "              'surfboard', 'tennis racket', 'bottle', 'N/A', 'wine glass', 'cup', 'fork',\n",
        "              'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange',\n",
        "              'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch',\n",
        "              'potted plant', 'bed', 'N/A', 'dining table', 'N/A', 'N/A', 'toilet',\n",
        "              'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave',\n",
        "              'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book', 'clock', 'vase',\n",
        "              'scissors', 'teddy bear', 'hair drier', 'toothbrush']\n",
        "\n",
        "len(coco_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JfXLUMmKrqfC",
        "outputId": "87ca5ae9-9ae5-45e9-f8d6-5d6d33e10250"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "91"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RqoM5rLfL3ZI"
      },
      "outputs": [],
      "source": [
        "#prediction function for SSD model\n",
        "coco_names = ['__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', \\\n",
        "              'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', \n",
        "              'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', \n",
        "              'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella',\n",
        "              'N/A', 'N/A', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard',\n",
        "              'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard',\n",
        "              'surfboard', 'tennis racket', 'bottle', 'N/A', 'wine glass', 'cup', 'fork',\n",
        "              'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange',\n",
        "              'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch',\n",
        "              'potted plant', 'bed', 'N/A', 'dining table', 'N/A', 'N/A', 'toilet',\n",
        "              'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave',\n",
        "              'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book', 'clock', 'vase',\n",
        "              'scissors', 'teddy bear', 'hair drier', 'toothbrush']\n",
        "\n",
        "\n",
        "def predict(input_tensor, model, device, detection_threshold):\n",
        "    outputs = model(input_tensor)\n",
        "    pred_classes = [coco_names[i] for i in outputs[0]['labels'].cpu().numpy()]\n",
        "    pred_labels = outputs[0]['labels'].cpu().numpy()\n",
        "    pred_scores = outputs[0]['scores'].detach().cpu().numpy()\n",
        "    pred_bboxes = outputs[0]['boxes'].detach().cpu().numpy()\n",
        "    \n",
        "    boxes, classes, labels, indices = [], [], [], []\n",
        "    for index in range(len(pred_scores)):\n",
        "        if pred_scores[index] >= detection_threshold:\n",
        "            boxes.append(pred_bboxes[index].astype(np.int32))\n",
        "            classes.append(pred_classes[index])\n",
        "            labels.append(pred_labels[index])\n",
        "            indices.append(index)\n",
        "    boxes = np.int32(boxes)\n",
        "    return boxes, classes, labels, indices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oz8jwHP9qTSZ"
      },
      "source": [
        "## Evaluate Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sdK8elMyJkNU"
      },
      "outputs": [],
      "source": [
        "mae_deepgaze_list = []\n",
        "iou_deepgaze_list = []\n",
        "\n",
        "mae_gt_list = []\n",
        "iou_gt_list = []\n",
        "\n",
        "for batch_idx, (img, tensor, mask, img_path) in enumerate(train_loader):\n",
        "    print(img_path)\n",
        "    print(img_path.split(\"/\")[-1].split(\".\")[0])\n",
        "    # print(tensor.shape)\n",
        "    img = np.array(img)\n",
        "    img = cv2.resize(img, (512, 512))\n",
        "    img_arr = np.float32(img) / 255\n",
        "    if mask is not None:\n",
        "      mask_arr = np.float32(mask) / 255\n",
        "    filename = os.path.basename(img_path)\n",
        "    tensor = tensor.cuda()\n",
        "    ####################################\n",
        "    #CHANGE THE MODEL BEING PASSED IN PREDICT TO WHAT MODEL YOU WANT TO EVALUATE!!!!\n",
        "    ####################################\n",
        "    boxes, classes, labels, indices = predict(tensor, frcnn_model, DEVICE, 0.9)\n",
        "    targets = [FasterRCNNBoxScoreTarget(labels=labels, bounding_boxes=boxes)]\n",
        "\n",
        "    grayscale_cam = cam(tensor, targets=targets)\n",
        "    grayscale_cam = grayscale_cam[0, :]\n",
        "    cam_image = show_cam_on_image(img_arr, grayscale_cam, use_rgb=True)\n",
        "\n",
        "    # deepgaze\n",
        "    centerbias_template = np.load('centerbias_mit1003.npy')\n",
        "    centerbias = zoom(centerbias_template, (img.shape[0]/centerbias_template.shape[0], img.shape[1]/centerbias_template.shape[1]), order=0, mode='nearest')\n",
        "    centerbias -= logsumexp(centerbias)\n",
        "    centerbias_tensor = torch.tensor([centerbias]).to(DEVICE)\n",
        "\n",
        "    log_density_prediction = deepgaze_model(tensor, centerbias_tensor)\n",
        "    log_density_prediction_squeezed = log_density_prediction.squeeze()\n",
        "    out_deepgaze = resizer(np.exp(log_density_prediction_squeezed.detach().cpu().numpy()))\n",
        "    out_deepgaze = normalize_data(out_deepgaze)\n",
        "\n",
        "    #metrics calculation for deepgaze\n",
        "    mae_deepgaze = calcMAE(out_deepgaze, grayscale_cam)\n",
        "    mae_deepgaze_list.append(mae_deepgaze)\n",
        "\n",
        "    iou_deepgaze = calcIOU(out_deepgaze, grayscale_cam)\n",
        "    iou_deepgaze_list.append(iou_deepgaze)\n",
        "\n",
        "    if mask is not None:\n",
        "      #metrics calculation for ground truth\n",
        "      mae_gt = calcMAE(mask, grayscale_cam)\n",
        "      mae_gt_list.append(mae_gt)\n",
        "\n",
        "      iou_gt = calcIOU(mask, grayscale_cam)\n",
        "      iou_gt_list.append(iou_gt)\n",
        "\n",
        "    print(f\"file {filename} has MAE {mae_deepgaze} and IOU {iou_deepgaze} for DeepGaze. Progress {batch_idx+1}/{len(train_loader)}\")\n",
        "    if mask is not None:\n",
        "      print(f\"file {filename} has MAE {mae_gt} and IOU {iou_gt} for Ground Truth. Progress {batch_idx+1}/{len(train_loader)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcLe_Grr3qUG"
      },
      "source": [
        "# Graphs for Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_6Wpf8r73phs"
      },
      "outputs": [],
      "source": [
        "mean_mae_deepgaze = sum(mae_deepgze_list) / len(mae_deepgaze_list)\n",
        "print(f\"mean MAE for deepgaze is {mean_mae_deepgaze}\")\n",
        "\n",
        "mean_iou_deepgaze = sum(iou_deepgze_list) / len(iou_deepgaze_list)\n",
        "print(f\"mean IOU for deepgaze is {mean_iou_deepgaze}\")\n",
        "\n",
        "mean_mae_gt = sum(mae_gt_list) / len(mae_gt_list)\n",
        "print(f\"mean MAE for human attention masks is {mean_mae_gt}\")\n",
        "\n",
        "mean_iou_gt = sum(iou_gt_list) / len(iou_gt_list)\n",
        "print(f\"mean IOU for human attention masks is {mean_iou_gt}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "EpvTXXi83sLY",
        "outputId": "5eeade0d-e2d7-4836-be09-7cfae9267fb3"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.cla()\n",
        "plt.clf()\n",
        "plt.hist(mae_list, bins=20, density=True)\n",
        "plt.title(\"Model vs DeepGaze Mean Average Error Distribution\")\n",
        "plt.xlabel(\"MAE\")\n",
        "plt.savefig('/content/hist.png')\n",
        "\n",
        "plt.cla()\n",
        "plt.clf()\n",
        "plt.hist(iou_list, bins=20, density=True)\n",
        "plt.title(\"Model vs DeepGaze Mean IOU Distribution\")\n",
        "plt.xlabel(\"IOU\")\n",
        "plt.savefig('/content/iou.png')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "EvaluateSaliencyMaps.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}